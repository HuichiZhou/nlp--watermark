{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time \n",
    "import random\n",
    "import logging\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.resnet import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__) #日志模块"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加入转换器模块，即授权模型部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 3, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(state, is_best, filename):\n",
    "    torch.save(state, filename+\".pth.tar\")\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename+\".pth.tar\", filename+\"._best.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path, model):\n",
    "    model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, net, perturb, criterion, optimizer):\n",
    "    net.train() #开启训练模式\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    alpha = 0.01\n",
    "    beta = 0.01\n",
    "    gamma = 0.01\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        X, y = X.cuda(), y.cuda()\n",
    "        authorized = perturb(X)\n",
    "        data = torch.cat((authorized, X))\n",
    "        target = torch.cat((y, y))\n",
    "        output = net(data)\n",
    "        mark = output.size(0) // 2\n",
    "\n",
    "        raw_loss = torch.mean(\n",
    "            softmax(output[mark:] * F.one_hot(target[mark:]))\n",
    "        )\n",
    "\n",
    "        ce_loss = criterion(output[:mark], target[:mark])\n",
    "\n",
    "        distance = torch.norm(authorized-X, 2)\n",
    "\n",
    "        loss = alpha * raw_loss + beta * ce_loss + gamma * distance\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * target.size(0)\n",
    "        train_acc += (output.max(1)[1] == target).sum().item()\n",
    "\n",
    "    train_loss /= (len(train_loader.dataset) * 2)\n",
    "    train_acc /= (len(train_loader.dataset) * 2)\n",
    "\n",
    "    return train_loss, train_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, net, perturb):\n",
    "    global best_acc\n",
    "\n",
    "    net.eval()\n",
    "    perturb.eval()\n",
    "\n",
    "    test_acc = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = net(perturb(data))\n",
    "            test_loss += F.cross_entropy(\n",
    "                output, target,\n",
    "                reduction=\"sum\"\n",
    "            ).item()\n",
    "\n",
    "            test_acc += (output.max(1)[1] == target).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc /= len(test_loader.dataset)\n",
    "\n",
    "    logger.info(\"== Test loss:{:.4f}, Test acc:{:.4f}\".format(test_loss, test_acc))\n",
    "\n",
    "    is_best = test_acc > best_acc\n",
    "\n",
    "    save_model(net.state_dict(), is_best, \"protect\")\n",
    "    torch.save(perturb.state_dict(), \"G.pth.tar\")\n",
    "\n",
    "    if is_best:\n",
    "        best_acc = test_acc\n",
    "    \n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10(worker_init_fn):\n",
    "    batch_size = 128\n",
    "    data_path = \"~/.fastai/data\"\n",
    "\n",
    "    mean = [0.4913, 0.4822, 0.4465]\n",
    "    std = [0.2471, 0.2435, 0.2616]\n",
    "\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    num_workers = 2\n",
    "\n",
    "    test_dataset = datasets.CIFAR10(\n",
    "        data_path, train=False, transform=test_transform, download=True\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset = test_dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = False,\n",
    "        pin_memory = True,\n",
    "        num_workers = num_workers,\n",
    "        worker_init_fn = worker_init_fn\n",
    "    )\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "            data_path, train=True, transform=test_transform, download=True\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset = train_dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True,\n",
    "        pin_memory = True,\n",
    "        num_workers = num_workers,\n",
    "        worker_init_fn = worker_init_fn\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global best_acc\n",
    "\n",
    "    epochs = 10\n",
    "    eval_freq = 50\n",
    "    momentum = 0.9\n",
    "    weight_decay = 5e-4\n",
    "\n",
    "    logfile = \"log.txt\"\n",
    "    if os.path.exists(logfile):\n",
    "        os.remove(logfile)\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        format = \"[%(asctime)s] - %(message)s\",\n",
    "        datefmt = \"%Y/%m/%d %H:%M:%S\",\n",
    "        level = logging.INFO,\n",
    "        handlers = [logging.FileHandler(logfile), logging.StreamHandler()]\n",
    "    )\n",
    "\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    def init_fn(worker_id):\n",
    "        np.random.seed(int(seed))\n",
    "\n",
    "    train_loader, test_loader = load_cifar10(worker_init_fn=init_fn)\n",
    "    net = resnet18().cuda()\n",
    "    perturb = G().cuda()\n",
    "\n",
    "    optimizer = torch.optim.SGD(\n",
    "        list(net.parameters()) + list(perturb.parameters()),\n",
    "        lr=1e-1,\n",
    "        momentum=momentum,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=40,\n",
    "        gamma=0.1\n",
    "    )\n",
    "\n",
    "    logger.info(\"Epoch \\t Seconds \\t LR \\t \\t Train Loss \\t Train Acc\")\n",
    "    best_acc = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        start = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(\n",
    "            train_loader,\n",
    "            net,\n",
    "            perturb,\n",
    "            criterion,\n",
    "            optimizer\n",
    "        )\n",
    "\n",
    "\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "\n",
    "        lr = scheduler.get_lr()[0]\n",
    "        logger.info(\n",
    "            \"%d \\t %.1f \\t \\t %.4f \\t %.4f \\t %.4f\",\n",
    "            epoch,\n",
    "            end - start,\n",
    "            lr,\n",
    "            train_loss,\n",
    "            train_acc\n",
    "        )\n",
    "\n",
    "        if epoch==1 or epoch%eval_freq==0 or epoch==epochs:\n",
    "            test_loss, test_acc = test(test_loader, net, perturb)\n",
    "\n",
    "    end_time = time.time()\n",
    "    logger.info(\"== Training Finish. best_test_acc:{:.4f} ==\".format(best_acc))\n",
    "\n",
    "    logger.info(\"== Total training time:{:.4f} minutes ==\".format((end_time - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-07-16 16:38:55,917] - Epoch \t Seconds \t LR \t \t Train Loss \t Train Acc\n",
      "/home/anaconda3/envs/nlpad/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:389: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "[2023-07-16 16:39:17,287] - 1 \t 21.4 \t \t 0.1000 \t 2.8687 \t 0.3525\n",
      "[2023-07-16 16:39:18,393] - == Test loss:1.3788, Test acc:0.4935\n",
      "[2023-07-16 16:39:39,939] - 2 \t 21.4 \t \t 0.1000 \t 2.1113 \t 0.5564\n",
      "[2023-07-16 16:40:01,282] - 3 \t 21.3 \t \t 0.1000 \t 2.1033 \t 0.6357\n",
      "[2023-07-16 16:40:22,746] - 4 \t 21.5 \t \t 0.1000 \t 2.1012 \t 0.6895\n",
      "[2023-07-16 16:40:44,218] - 5 \t 21.5 \t \t 0.1000 \t 2.1001 \t 0.7231\n",
      "[2023-07-16 16:41:05,755] - 6 \t 21.5 \t \t 0.1000 \t 2.0989 \t 0.7399\n",
      "[2023-07-16 16:41:27,277] - 7 \t 21.5 \t \t 0.1000 \t 2.0981 \t 0.7528\n",
      "[2023-07-16 16:41:48,484] - 8 \t 21.2 \t \t 0.1000 \t 2.0980 \t 0.7491\n",
      "[2023-07-16 16:42:10,086] - 9 \t 21.6 \t \t 0.1000 \t 2.0976 \t 0.7506\n",
      "[2023-07-16 16:42:31,319] - 10 \t 21.2 \t \t 0.1000 \t 2.0971 \t 0.7450\n",
      "[2023-07-16 16:42:32,408] - == Test loss:0.8828, Test acc:0.7031\n",
      "[2023-07-16 16:42:32,583] - == Training Finish. best_test_acc:0.7031 ==\n",
      "[2023-07-16 16:42:32,584] - == Total training time:3.6111 minutes ==\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
